{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import DatasetDict, Dataset, load_from_disk\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.evaluation.answer_extractor import extracted_answers, extracted_answers_convfinqa\n",
    "from src.evaluation.answer_extractor_en import extracted_answers_ner, extracted_answers_finred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "data_folder = '/home/sandernoels/fingeit/data/final/responses/final/en-'\n",
    "client = OpenAI(api_key='<token_here>')\n",
    "\n",
    "models = {\n",
    "    'fingeit' : '/home/sandernoels/fingeit/data/final/responses/en/en_FinGEITje-sft_responses_e0f909d4-e04d-4441-86a3-5e529f007d53.txt',\n",
    "    'fingpt-llama' : '/home/sandernoels/fingeit/data/final/responses/en/en_fingpt_llama2_responses_c53d172a-dc48-4ea9-86c5-0063e9ff10a1.txt',\n",
    "    'pixiu' : '/home/sandernoels/fingeit/data/final/responses/en/en_pixiu_responses_711bf9bc-13e3-4273-9ad7-568749105fcd.txt',\n",
    "}\n",
    "\n",
    "eval_df = load_from_disk('/home/sandernoels/fingeit/data/final/en_sampled_eval_df_ext')['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, path in models.items():\n",
    "    eval_df[f'prediction_{model}_raw'] = read_lines(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-sentiment'\n",
    "\n",
    "sentiment_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers(sentiment_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    sentiment_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_sentiment import SentimentEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in sentiment.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = sentiment.copy()\n",
    "    df[col] = df[col].astype(str)\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_sentiment_score = SentimentEvaluator(language = 'EN')._evaluate(new_test_ds['test'])\n",
    "    eval_sentiment_score.metrics\n",
    "\n",
    "    evals[col] = eval_sentiment_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.728</td>\n",
       "      <td>0.731204</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.728553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.728</td>\n",
       "      <td>0.731204</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.728553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingeit</th>\n",
       "      <td>0.692</td>\n",
       "      <td>0.697012</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.692340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingeit_raw</th>\n",
       "      <td>0.684</td>\n",
       "      <td>0.686843</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.682951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingpt-llama_raw</th>\n",
       "      <td>0.402</td>\n",
       "      <td>0.210271</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.247337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingpt-llama</th>\n",
       "      <td>0.402</td>\n",
       "      <td>0.210545</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.247659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_pixiu_raw            0.728  0.731204     0.728     0.728553\n",
       "prediction_pixiu                0.728  0.731204     0.728     0.728553\n",
       "prediction_en-fingeit           0.692  0.697012     0.692     0.692340\n",
       "prediction_en-fingeit_raw       0.684  0.686843     0.684     0.682951\n",
       "prediction_en-fingpt-llama_raw  0.402  0.210271     0.402     0.247337\n",
       "prediction_en-fingpt-llama      0.402  0.210545     0.402     0.247659"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-headline'\n",
    "\n",
    "headline_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers(headline_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    headline_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_headline import HeadlineEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in headline.columns if el.startswith('prediction')]\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = headline.copy()\n",
    "    df[col] = df[col].astype(str)\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_headline_score = HeadlineEvaluator(language = 'EN')._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_headline_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1 binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.884</td>\n",
       "      <td>0.797203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.884</td>\n",
       "      <td>0.797203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.204724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.204724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.638</td>\n",
       "      <td>0.290196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.366</td>\n",
       "      <td>0.189258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Acc  F1 binary\n",
       "prediction_pixiu             0.884   0.797203\n",
       "prediction_pixiu_raw         0.884   0.797203\n",
       "prediction_fingpt-llama      0.798   0.204724\n",
       "prediction_fingpt-llama_raw  0.798   0.204724\n",
       "prediction_fingeit           0.638   0.290196\n",
       "prediction_fingeit_raw       0.366   0.189258"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "\n",
    "df.sort_values(by='Acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-ner'\n",
    "\n",
    "ner_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers_ner(ner_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    ner_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner import NEREvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "prediction_cols = [el for el in ner.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = ner.copy()\n",
    "    df[col] = df[col].astype(str).fillna('nan')\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_ner_score = NEREvaluator(language = 'EN')._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_ner_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.675768</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingeit</th>\n",
       "      <td>0.483986</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingpt-llama</th>\n",
       "      <td>0.221198</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingpt-llama_raw</th>\n",
       "      <td>0.172249</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingeit_raw</th>\n",
       "      <td>0.021164</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      F1  \\\n",
       "prediction_pixiu                0.675768   \n",
       "prediction_en-fingeit           0.483986   \n",
       "prediction_en-fingpt-llama      0.221198   \n",
       "prediction_en-fingpt-llama_raw  0.172249   \n",
       "prediction_en-fingeit_raw       0.021164   \n",
       "prediction_pixiu_raw                 0.0   \n",
       "\n",
       "                                                            Classification Report  \n",
       "prediction_pixiu                              precision    recall  f1-score   ...  \n",
       "prediction_en-fingeit                         precision    recall  f1-score   ...  \n",
       "prediction_en-fingpt-llama                    precision    recall  f1-score   ...  \n",
       "prediction_en-fingpt-llama_raw                precision    recall  f1-score   ...  \n",
       "prediction_en-fingeit_raw                     precision    recall  f1-score   ...  \n",
       "prediction_pixiu_raw                          precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER (CLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-ner-cls'\n",
    "\n",
    "ner_cls_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers(ner_cls_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client,['person', 'organization', 'location'])\n",
    "    ner_cls_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_cls_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner_cls import NERCLSEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_cls = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in ner_cls.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = ner_cls.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_ner_cls_classification_score = NERCLSEvaluator('EN')._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_ner_cls_classification_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.880</td>\n",
       "      <td>0.874789</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.882861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.548698</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.847469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.492</td>\n",
       "      <td>0.484067</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.463851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.484</td>\n",
       "      <td>0.469738</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.442258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.432</td>\n",
       "      <td>0.405270</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.365428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.402</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.309451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit           0.880  0.874789     0.880     0.882861\n",
       "prediction_fingeit_raw       0.806  0.548698     0.806     0.847469\n",
       "prediction_fingpt-llama      0.492  0.484067     0.492     0.463851\n",
       "prediction_pixiu             0.484  0.469738     0.484     0.442258\n",
       "prediction_fingpt-llama_raw  0.432  0.405270     0.432     0.365428\n",
       "prediction_pixiu_raw         0.402  0.360759     0.402     0.309451"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-finred'\n",
    "\n",
    "finred_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers_finred(finred_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    finred_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred_base.to_csv(f'{data_folder}{task}-cls.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_finred import FinRedEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred = pd.read_csv(f'{data_folder}{task}-cls.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in finred.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = finred.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_finred_classification_score = FinRedEvaluator(language='EN')._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_finred_classification_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.478076</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.629823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.464294</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>0.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingeit</th>\n",
       "      <td>0.308772</td>\n",
       "      <td>0.197892</td>\n",
       "      <td>0.308772</td>\n",
       "      <td>0.332644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_en-fingeit_raw</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.096145</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.115203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.044343</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.084766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.006798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingpt-llama      0.614035  0.478076  0.614035     0.629823\n",
       "prediction_fingpt-llama_raw  0.589474  0.464294  0.589474     0.612903\n",
       "prediction_en-fingeit        0.308772  0.197892  0.308772     0.332644\n",
       "prediction_en-fingeit_raw    0.105263  0.096145  0.105263     0.115203\n",
       "prediction_pixiu             0.087719  0.044343  0.087719     0.084766\n",
       "prediction_pixiu_raw         0.003509  0.002083  0.003509     0.006798"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvFinQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-convfinqa'\n",
    "\n",
    "convfinqa_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers_convfinqa(convfinqa_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    convfinqa_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "convfinqa_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_convfinqa import ConvFinQaEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "convfinqa = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in convfinqa.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = convfinqa.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_convfinqa_score = ConvFinQaEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_convfinqa_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.494781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.004292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Accuracy\n",
       "prediction_pixiu_raw         0.494781\n",
       "prediction_pixiu             0.466000\n",
       "prediction_fingeit_raw       0.420000\n",
       "prediction_fingeit           0.408000\n",
       "prediction_fingpt-llama_raw  0.004292\n",
       "prediction_fingpt-llama      0.004000"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='Accuracy', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
