{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from seqeval.metrics import f1_score, classification_report, accuracy_score\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_processing.construct_messages import ALPACA_INTROMESSAGE_INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Task      | Metric |\n",
    "|-----------|----------------------|\n",
    "| Classification | Accuracy |\n",
    "| Classification | F1 Score |\n",
    "| Classification | Missing Ratio |\n",
    "| Classification | Matthews Correlation Coefficient (MCC) |\n",
    "| Sequential Labeling | F1 score |\n",
    "| Sequential Labeling | Label F1 score |\n",
    "| Relation Extraction | Precision |\n",
    "| Relation Extraction | Recall |\n",
    "| Relation Extraction | F1 score |\n",
    "| Extractive and Abstractive Summarization | Rouge-N |\n",
    "| Extractive and Abstractive Summarization | Rouge-L |\n",
    "| Question Answering | EmACC |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data      | Task |\n",
    "|-------------|-----------------|\n",
    "| FPB | sentiment analysis |\n",
    "| FiQA-SA | sentiment analysis |\n",
    "| Headline | news headline classification |\n",
    "| NER | named entity recognition |\n",
    "| FinQA | question answering |\n",
    "| ConvFinQA | question answering |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='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')\n",
    "\n",
    "def get_prediction(prompt):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt,\n",
    "          }\n",
    "      ],\n",
    "      model=\"gpt-3.5-turbo\",    \n",
    "      temperature=0,        \n",
    "      max_tokens=2048         \n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def get_prediction_on_message(message):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=message,\n",
    "      model=\"gpt-3.5-turbo\",    \n",
    "      temperature=0,        \n",
    "      max_tokens=2048         \n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def get_prompt(instruction,input):\n",
    "    return ALPACA_INTROMESSAGE_INPUT.replace('{instruction}',instruction).replace('{input}',input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final ConvFinQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_convfinqa import ConvFinQaEvaluator\n",
    "from src.data_processing.construct_messages import add_prediction_messages\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add additional information to the message\n",
    "\n",
    "def append_string(examples):\n",
    "    return {'instruction': [instruction + ' - answer the question by just providing the number, nothing else' for instruction in examples['instruction']]}\n",
    "\n",
    "def get_predictions(path, append_instruction = False):\n",
    "\n",
    "    ds = load_from_disk(path)\n",
    "    if append_instruction:\n",
    "        test_ds = ds['test'].map(append_string)\n",
    "    else:\n",
    "        test_ds = ds['test']\n",
    "\n",
    "    updated_dataset = test_ds.map(add_prediction_messages)\n",
    "    test_ds = updated_dataset.to_pandas().head(30)\n",
    "\n",
    "    test_ds['prediction'] = test_ds.messages.apply(lambda x : get_prediction_on_message(x))\n",
    "\n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(test_ds)\n",
    "\n",
    "    return new_test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1453/1453 [00:00<00:00, 1480.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_convfinqa = get_predictions('../data/final_filtered/fingpt-convfinqa', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "eval_convfinqa_score = ConvFinQaEvaluator('CohereForAI/aya-101')._evaluate(eval_convfinqa['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='Accuracy', value=0.26666666666666666)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_convfinqa_score.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_sentiment import SentimentEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5788/5788 [00:00<00:00, 21870.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_sentiment = get_predictions('../data/final_filtered/fingpt-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "eval_sentiment_score = SentimentEvaluator('CohereForAI/aya-101')._evaluate(eval_sentiment['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='acc', value=0.5333333333333333),\n",
       " Metric(name='f1_macro', value=0.5438095238095239),\n",
       " Metric(name='f1_micro', value=0.5333333333333333),\n",
       " Metric(name='f1_weighted', value=0.5251428571428571)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentiment_score.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_headline import HeadlineEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9094/9094 [00:00<00:00, 23099.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_headline = get_predictions('../data/final_filtered/fingpt-headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_headline_score = HeadlineEvaluator('CohereForAI/aya-101')._evaluate(eval_headline['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='Acc', value=0.5),\n",
       " Metric(name='F1 binary', value=0.4827586206896552)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_headline_score.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner import NEREvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 68/68 [00:00<00:00, 10151.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_ner = get_predictions('../data/final_filtered/fingpt-ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_ner_score = NEREvaluator('CohereForAI/aya-101')._evaluate(eval_ner['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='Accuracy', value=0.9504256384576866),\n",
       " Metric(name='F1', value=0.0),\n",
       " Metric(name='Classification Report', value='              precision    recall  f1-score   support\\n\\n         LOC       0.00      0.00      0.00        10\\n         ORG       0.00      0.00      0.00         5\\n         PER       0.00      0.00      0.00        66\\n\\n   micro avg       0.00      0.00      0.00        81\\n   macro avg       0.00      0.00      0.00        81\\nweighted avg       0.00      0.00      0.00        81\\n')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ner_score.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingeitje",
   "language": "python",
   "name": "fingeitje"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
