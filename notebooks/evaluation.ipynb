{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from openai import OpenAI\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.evaluation.llm_extractor import extracted_answers, extracted_answers_ner, extracted_answers_finred, extracted_answers_convfinqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "data_folder = '/home/sandernoels/fingeit/data/'\n",
    "client = OpenAI(api_key='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')\n",
    "\n",
    "models = {\n",
    "    'fingeit' : '/home/sandernoels/fingeit/data/responses/fingeitje_responses_1e8277ee-acdf-48bf-9f3e-ba538c2d22d3.txt',\n",
    "    'geitje-ultra' : '/home/sandernoels/fingeit/data/responses/GEITje-7B-ultra_responses_6787f010-26bd-4757-97db-106a67e67411.txt',\n",
    "    'geitje' : '/home/sandernoels/fingeit/data/responses/GEITje-7B-chat-v2_responses_0d329199-13e8-4620-b0a4-eb6a9cf20913.txt',\n",
    "    'fingpt-llama' : '/home/sandernoels/fingeit/data/responses/fingpt_llama2_responses_897d1bc3-9a03-4a94-abb9-235f9ad150ec.txt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-sentiment'\n",
    "\n",
    "model_name = list(models.keys())[3]\n",
    "path = list(models.values())[3]\n",
    "\n",
    "sentiment = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers(sentiment.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "sentiment[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_sentiment import SentimentEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in sentiment.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = sentiment.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_sentiment_score = SentimentEvaluator()._evaluate(new_test_ds['test'])\n",
    "    eval_sentiment_score.metrics\n",
    "\n",
    "    evals[col] = eval_sentiment_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.783196</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.783196</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.674</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.661714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.564</td>\n",
       "      <td>0.529673</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.555212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.540</td>\n",
       "      <td>0.497929</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.520434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.454</td>\n",
       "      <td>0.448226</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.466206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.320940</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.276341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.268</td>\n",
       "      <td>0.140904</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.113287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit           0.790  0.783196     0.790     0.790308\n",
       "prediction_fingeit_raw       0.790  0.783196     0.790     0.790308\n",
       "prediction_geitje-ultra      0.674  0.638646     0.674     0.661714\n",
       "prediction_geitje-ultra_raw  0.564  0.529673     0.564     0.555212\n",
       "prediction_geitje            0.540  0.497929     0.540     0.520434\n",
       "prediction_geitje_raw        0.454  0.448226     0.454     0.466206\n",
       "prediction_fingpt-llama      0.350  0.320940     0.350     0.276341\n",
       "prediction_fingpt-llama_raw  0.268  0.140904     0.268     0.113287"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-headline'\n",
    "\n",
    "model_name = list(models.keys())[3]\n",
    "path = list(models.values())[3]\n",
    "\n",
    "headline = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers(headline.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "headline[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_headline import HeadlineEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in headline.columns if el.startswith('prediction')]\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = headline.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_headline_score = HeadlineEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_headline_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1 binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.314</td>\n",
       "      <td>0.215103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.298</td>\n",
       "      <td>0.166271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.082</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.064</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Acc  F1 binary\n",
       "prediction_fingeit           0.920   0.836066\n",
       "prediction_fingeit_raw       0.920   0.836066\n",
       "prediction_fingpt-llama      0.696   0.000000\n",
       "prediction_geitje_raw        0.314   0.215103\n",
       "prediction_geitje            0.298   0.166271\n",
       "prediction_geitje-ultra_raw  0.082   0.068966\n",
       "prediction_geitje-ultra      0.064   0.025000\n",
       "prediction_fingpt-llama_raw  0.000   0.000000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "\n",
    "df.sort_values(by='Acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-ner'\n",
    "\n",
    "model_name = list(models.keys())[0]\n",
    "path = list(models.values())[0]\n",
    "\n",
    "ner = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers_ner(ner.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "ner[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner import NEREvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "prediction_cols = [el for el in ner.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = ner.copy()\n",
    "    df[col] = df[col].astype(str).fillna('nan')\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_ner_score = NEREvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_ner_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.432836</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.417266</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.154472</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.099585</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.010695</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   F1  \\\n",
       "prediction_fingeit_raw       0.432836   \n",
       "prediction_fingeit           0.417266   \n",
       "prediction_geitje            0.154472   \n",
       "prediction_geitje-ultra      0.099585   \n",
       "prediction_fingpt-llama      0.010695   \n",
       "prediction_geitje-ultra_raw       0.0   \n",
       "prediction_geitje_raw             0.0   \n",
       "prediction_fingpt-llama_raw       0.0   \n",
       "\n",
       "                                                         Classification Report  \n",
       "prediction_fingeit_raw                     precision    recall  f1-score   ...  \n",
       "prediction_fingeit                         precision    recall  f1-score   ...  \n",
       "prediction_geitje                          precision    recall  f1-score   ...  \n",
       "prediction_geitje-ultra                    precision    recall  f1-score   ...  \n",
       "prediction_fingpt-llama                    precision    recall  f1-score   ...  \n",
       "prediction_geitje-ultra_raw                precision    recall  f1-score   ...  \n",
       "prediction_geitje_raw                      precision    recall  f1-score   ...  \n",
       "prediction_fingpt-llama_raw                precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-finred'\n",
    "\n",
    "model_name = list(models.keys())[0]\n",
    "path = list(models.values())[0]\n",
    "\n",
    "finred = pd.read_csv(f'{data_folder}{task}-classification.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers_finred(finred.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "finred[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred.to_csv(f'{data_folder}{task}-classification.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_finred import FinRedEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred = pd.read_csv(f'{data_folder}{task}-classification.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in finred.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = finred.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_finred_classification_score = FinRedEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_finred_classification_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.468741</td>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.559711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.479854</td>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.557941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.179006</td>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.232336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.123494</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>0.123494</td>\n",
       "      <td>0.147762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.078931</td>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.121220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.014581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit_raw       0.569277  0.468741  0.569277     0.559711\n",
       "prediction_fingeit           0.569277  0.479854  0.569277     0.557941\n",
       "prediction_fingpt-llama      0.259036  0.179006  0.259036     0.232336\n",
       "prediction_geitje            0.123494  0.104194  0.123494     0.147762\n",
       "prediction_geitje-ultra      0.111446  0.078931  0.111446     0.121220\n",
       "prediction_fingpt-llama_raw  0.021084  0.028900  0.021084     0.014581\n",
       "prediction_geitje_raw        0.000000  0.000000  0.000000     0.000000\n",
       "prediction_geitje-ultra_raw  0.000000  0.000000  0.000000     0.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvFinQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-convfinqa'\n",
    "\n",
    "model_name = list(models.keys())[0]\n",
    "path = list(models.values())[0]\n",
    "\n",
    "convfinqa = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers_convfinqa(convfinqa.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "convfinqa[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "convfinqa.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_convfinqa import ConvFinQaEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "convfinqa = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in convfinqa.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = convfinqa.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_convfinqa_score = ConvFinQaEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_convfinqa_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.010554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Accuracy\n",
       "prediction_fingeit_raw       0.324000\n",
       "prediction_fingeit           0.324000\n",
       "prediction_geitje            0.056000\n",
       "prediction_geitje_raw        0.056000\n",
       "prediction_geitje-ultra      0.036000\n",
       "prediction_geitje-ultra_raw  0.016000\n",
       "prediction_fingpt-llama_raw  0.010554\n",
       "prediction_fingpt-llama      0.008000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(instruction, input):    \n",
    "    return [{'content': 'Je bent een behulpzame financiÃ«le assistent. help met zorg, respect en waarheid. Reageer met de grootste nuttigheid maar wel veilig. Vermijd schadelijke, onethische, bevooroordeelde of negatieve inhoud. Zorg ervoor dat antwoorden eerlijkheid en positiviteit promoten.',\n",
    "    'role': 'system'},\n",
    "    {'content': f'Hieronder staat een instructie die een taak beschrijft, samen met een input die context voorziet\\nSchrijf een reactie die op een passende manier voldoet aan de vraag.\\n\\n\\n### Instructie:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Reactie:\\n',\n",
    "    'role': 'user'}]\n",
    "\n",
    "def get_prediction_on_message(message, client):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=message,\n",
    "      model=\"gpt-3.5-turbo\",    \n",
    "      temperature=0,        \n",
    "      max_tokens=4096         \n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-sentiment'\n",
    "\n",
    "sentiment = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "sentiment['messages'] = sentiment.apply(lambda x : create_message(x['instruction'],x['input']), axis = 1)\n",
    "sentiment[f'prediction_{model_name}_raw'] = sentiment['messages'].apply(lambda x : get_prediction_on_message(x,client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers(sentiment.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "sentiment[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in sentiment.columns if el.startswith('prediction')]\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = sentiment.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_sentiment_score = SentimentEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_sentiment_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.783196</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.783196</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.724415</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.740101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.742</td>\n",
       "      <td>0.713072</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.729132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.674</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.661714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.564</td>\n",
       "      <td>0.529673</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.555212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.540</td>\n",
       "      <td>0.497929</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.520434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.454</td>\n",
       "      <td>0.448226</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.466206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.320940</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.276341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.268</td>\n",
       "      <td>0.140904</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.113287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit_raw        0.790  0.783196     0.790     0.790308\n",
       "prediction_fingeit            0.790  0.783196     0.790     0.790308\n",
       "prediction_gpt-3.5-turbo      0.752  0.724415     0.752     0.740101\n",
       "prediction_gpt-3.5-turbo_raw  0.742  0.713072     0.742     0.729132\n",
       "prediction_geitje-ultra       0.674  0.638646     0.674     0.661714\n",
       "prediction_geitje-ultra_raw   0.564  0.529673     0.564     0.555212\n",
       "prediction_geitje             0.540  0.497929     0.540     0.520434\n",
       "prediction_geitje_raw         0.454  0.448226     0.454     0.466206\n",
       "prediction_fingpt-llama       0.350  0.320940     0.350     0.276341\n",
       "prediction_fingpt-llama_raw   0.268  0.140904     0.268     0.113287"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-headline'\n",
    "\n",
    "headline = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "headline['messages'] = headline.apply(lambda x : create_message(x['instruction'],x['input']), axis = 1)\n",
    "headline[f'prediction_{model_name}_raw'] = headline['messages'].apply(lambda x : get_prediction_on_message(x,client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers(headline.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "headline[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in headline.columns if el.startswith('prediction')]\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = headline.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_headline_score = HeadlineEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_headline_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1 binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.640</td>\n",
       "      <td>0.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.606</td>\n",
       "      <td>0.466125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.314</td>\n",
       "      <td>0.215103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.298</td>\n",
       "      <td>0.166271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.082</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.064</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Acc  F1 binary\n",
       "prediction_fingeit            0.920   0.836066\n",
       "prediction_fingeit_raw        0.920   0.836066\n",
       "prediction_fingpt-llama       0.696   0.000000\n",
       "prediction_gpt-3.5-turbo      0.640   0.485714\n",
       "prediction_gpt-3.5-turbo_raw  0.606   0.466125\n",
       "prediction_geitje_raw         0.314   0.215103\n",
       "prediction_geitje             0.298   0.166271\n",
       "prediction_geitje-ultra_raw   0.082   0.068966\n",
       "prediction_geitje-ultra       0.064   0.025000\n",
       "prediction_fingpt-llama_raw   0.000   0.000000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "\n",
    "df.sort_values(by='Acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-ner'\n",
    "\n",
    "ner = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "ner['messages'] = ner.apply(lambda x : create_message(x['instruction'],x['input']), axis = 1)\n",
    "ner[f'prediction_{model_name}_raw'] = ner['messages'].apply(lambda x : get_prediction_on_message(x,client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers_ner(ner.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "ner[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "prediction_cols = [el for el in ner.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = ner.copy()\n",
    "    df[col] = df[col].astype(str).fillna('nan')\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_ner_score = NEREvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_ner_score.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.432836</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.417266</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.315217</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.154472</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.099585</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.010695</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    F1  \\\n",
       "prediction_fingeit_raw        0.432836   \n",
       "prediction_fingeit            0.417266   \n",
       "prediction_gpt-3.5-turbo      0.315217   \n",
       "prediction_geitje             0.154472   \n",
       "prediction_geitje-ultra       0.099585   \n",
       "prediction_fingpt-llama       0.010695   \n",
       "prediction_geitje-ultra_raw        0.0   \n",
       "prediction_geitje_raw              0.0   \n",
       "prediction_fingpt-llama_raw        0.0   \n",
       "prediction_gpt-3.5-turbo_raw       0.0   \n",
       "\n",
       "                                                          Classification Report  \n",
       "prediction_fingeit_raw                      precision    recall  f1-score   ...  \n",
       "prediction_fingeit                          precision    recall  f1-score   ...  \n",
       "prediction_gpt-3.5-turbo                    precision    recall  f1-score   ...  \n",
       "prediction_geitje                           precision    recall  f1-score   ...  \n",
       "prediction_geitje-ultra                     precision    recall  f1-score   ...  \n",
       "prediction_fingpt-llama                     precision    recall  f1-score   ...  \n",
       "prediction_geitje-ultra_raw                 precision    recall  f1-score   ...  \n",
       "prediction_geitje_raw                       precision    recall  f1-score   ...  \n",
       "prediction_fingpt-llama_raw                 precision    recall  f1-score   ...  \n",
       "prediction_gpt-3.5-turbo_raw                precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-finred'\n",
    "\n",
    "finred = pd.read_csv(f'{data_folder}{task}-classification.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "finred['messages'] = finred.apply(lambda x : create_message(x['instruction'],x['input']), axis = 1)\n",
    "finred[f'prediction_{model_name}_raw'] = finred['messages'].apply(lambda x : get_prediction_on_message(x,client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers_finred(finred.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "finred[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred.to_csv(f'{data_folder}{task}-classification.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in finred.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = finred.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_finred_classification_score = FinRedEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_finred_classification_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.468741</td>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.559711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.479854</td>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.557941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.179006</td>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.232336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.136517</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.155826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.123494</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>0.123494</td>\n",
       "      <td>0.147762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.078931</td>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.121220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.014581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit_raw        0.569277  0.468741  0.569277     0.559711\n",
       "prediction_fingeit            0.569277  0.479854  0.569277     0.557941\n",
       "prediction_fingpt-llama       0.259036  0.179006  0.259036     0.232336\n",
       "prediction_gpt-3.5-turbo      0.156627  0.136517  0.156627     0.155826\n",
       "prediction_geitje             0.123494  0.104194  0.123494     0.147762\n",
       "prediction_geitje-ultra       0.111446  0.078931  0.111446     0.121220\n",
       "prediction_fingpt-llama_raw   0.021084  0.028900  0.021084     0.014581\n",
       "prediction_geitje-ultra_raw   0.000000  0.000000  0.000000     0.000000\n",
       "prediction_geitje_raw         0.000000  0.000000  0.000000     0.000000\n",
       "prediction_gpt-3.5-turbo_raw  0.000000  0.000000  0.000000     0.000000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvFinQA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-convfinqa'\n",
    "\n",
    "convfinqa = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "convfinqa['messages'] = convfinqa.apply(lambda x : create_message(x['instruction'],x['input']), axis = 1)\n",
    "convfinqa[f'prediction_{model_name}_raw'] = convfinqa['messages'].apply(lambda x : get_prediction_on_message(x,client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based extraction\n",
    "extracted_answer = extracted_answers_convfinqa(convfinqa.rename(columns={f'prediction_{model_name}_raw' : 'prediction'}), client)\n",
    "convfinqa[f'prediction_{model_name}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convfinqa.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in convfinqa.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = convfinqa.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_convfinqa_score = ConvFinQaEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_convfinqa_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.010554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy\n",
       "prediction_fingeit_raw        0.324000\n",
       "prediction_fingeit            0.324000\n",
       "prediction_gpt-3.5-turbo      0.244000\n",
       "prediction_gpt-3.5-turbo_raw  0.196000\n",
       "prediction_geitje             0.056000\n",
       "prediction_geitje_raw         0.056000\n",
       "prediction_geitje-ultra       0.036000\n",
       "prediction_geitje-ultra_raw   0.016000\n",
       "prediction_fingpt-llama_raw   0.010554\n",
       "prediction_fingpt-llama       0.008000"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='Accuracy', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
