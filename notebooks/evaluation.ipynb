{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from openai import OpenAI\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_processing.construct_messages import ALPACA_INTROMESSAGE_INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Task      | Metric |\n",
    "|-----------|----------------------|\n",
    "| Classification | Accuracy |\n",
    "| Classification | F1 Score |\n",
    "| Classification | Missing Ratio |\n",
    "| Classification | Matthews Correlation Coefficient (MCC) |\n",
    "| Sequential Labeling | F1 score |\n",
    "| Sequential Labeling | Label F1 score |\n",
    "| Relation Extraction | Precision |\n",
    "| Relation Extraction | Recall |\n",
    "| Relation Extraction | F1 score |\n",
    "| Extractive and Abstractive Summarization | Rouge-N |\n",
    "| Extractive and Abstractive Summarization | Rouge-L |\n",
    "| Question Answering | EmACC |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data      | Task |\n",
    "|-------------|-----------------|\n",
    "| FPB | sentiment analysis |\n",
    "| FiQA-SA | sentiment analysis |\n",
    "| Headline | news headline classification |\n",
    "| NER | named entity recognition |\n",
    "| FinQA | question answering |\n",
    "| ConvFinQA | question answering |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='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')\n",
    "\n",
    "def get_prediction(prompt):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt,\n",
    "          }\n",
    "      ],\n",
    "      model=\"gpt-3.5-turbo\",    \n",
    "      temperature=0,        \n",
    "      max_tokens=2048         \n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def get_prediction_on_message(message):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=message,\n",
    "      model=\"gpt-3.5-turbo\",    \n",
    "      temperature=0,        \n",
    "      max_tokens=2048         \n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def get_prompt(instruction,input):\n",
    "    return ALPACA_INTROMESSAGE_INPUT.replace('{instruction}',instruction).replace('{input}',input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final ConvFinQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_convfinqa import ConvFinQaEvaluator\n",
    "from src.data_processing.construct_messages import add_prediction_messages\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add additional information to the message\n",
    "\n",
    "def get_predictions(path, task = None):\n",
    "\n",
    "    ds = load_from_disk(path)\n",
    "    if task:\n",
    "        test_ds = ds['test'].map(lambda x: {'instruction': x['instruction'] + task })\n",
    "    else:\n",
    "        test_ds = ds['test']\n",
    "\n",
    "    updated_dataset = test_ds.map(add_prediction_messages)\n",
    "    test_ds = updated_dataset.to_pandas().head(30)\n",
    "\n",
    "    test_ds['prediction'] = test_ds.messages.apply(lambda x : get_prediction_on_message(x))\n",
    "\n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(test_ds)\n",
    "\n",
    "    return new_test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1453/1453 [00:00<00:00, 1480.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "task =  '- antwoord op de vraag door juist te antwoorden met het getal, niets anders'\n",
    " \n",
    "eval_convfinqa = get_predictions('../data/final_filtered/fingpt-convfinqa', task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "eval_convfinqa_score = ConvFinQaEvaluator('CohereForAI/aya-101')._evaluate(eval_convfinqa['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='Accuracy', value=0.26666666666666666)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_convfinqa_score.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_sentiment import SentimentEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5788/5788 [00:00<00:00, 21870.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_sentiment = get_predictions('../data/final_filtered/fingpt-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "eval_sentiment_score = SentimentEvaluator('CohereForAI/aya-101')._evaluate(eval_sentiment['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='acc', value=0.5333333333333333),\n",
       " Metric(name='f1_macro', value=0.5438095238095239),\n",
       " Metric(name='f1_micro', value=0.5333333333333333),\n",
       " Metric(name='f1_weighted', value=0.5251428571428571)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentiment_score.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_headline import HeadlineEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9094/9094 [00:00<00:00, 23099.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_headline = get_predictions('../data/final_filtered/fingpt-headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_headline_score = HeadlineEvaluator('CohereForAI/aya-101')._evaluate(eval_headline['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='Acc', value=0.5),\n",
       " Metric(name='F1 binary', value=0.4827586206896552)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_headline_score.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner import NEREvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ' - antwoord op de vraag door te anwoorden in het volgende formaat: [] is een [], [] is een [], ...'\n",
    "\n",
    "eval_ner = get_predictions('../data/final_filtered/fingpt-ner', task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 6018.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# more tweaking needed\n",
    "eval_ner['test'] = eval_ner['test'].map(lambda x : {'prediction' : x['prediction'].replace('[','').replace(']','').replace('- ','').replace('\\n','').strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_ner_score = NEREvaluator('CohereForAI/aya-101')._evaluate(eval_ner['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Metric(name='Accuracy', value=0.9228843264897346),\n",
       " Metric(name='F1', value=0.18987341772151897),\n",
       " Metric(name='Classification Report', value='              precision    recall  f1-score   support\\n\\n         LOC       0.20      0.20      0.20        10\\n           O       0.00      0.00      0.00         0\\n         ORG       0.00      0.00      0.00         5\\n         PER       0.59      0.20      0.30        66\\n\\n   micro avg       0.19      0.19      0.19        81\\n   macro avg       0.20      0.10      0.12        81\\nweighted avg       0.51      0.19      0.27        81\\n')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ner_score.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingeitje",
   "language": "python",
   "name": "fingeitje"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
