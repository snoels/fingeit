{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import DatasetDict, Dataset, load_from_disk\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.evaluation.answer_extractor import extracted_answers, extracted_answers_ner, extracted_answers_finred, extracted_answers_convfinqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "data_folder = '/home/sandernoels/fingeit/data/final/responses/final/'\n",
    "client = OpenAI(api_key='<token_here>')\n",
    "\n",
    "models = {\n",
    "    'fingeit' : '/home/sandernoels/fingeit/data/final/responses/nl/FinGEITje-sft_responses_b72dde7c-ead1-4741-93ee-e089b49809d4.txt',\n",
    "    'geitje-ultra' : '/home/sandernoels/fingeit/data/final/responses/nl/GEITje-7B-ultra_responses_3a614c9f-de6f-44d8-b4a8-5debf1ea61c6.txt',\n",
    "    'geitje' : '/home/sandernoels/fingeit/data/final/responses/nl/GEITje-7B-ultra_responses_3a614c9f-de6f-44d8-b4a8-5debf1ea61c6.txt',\n",
    "    'fingpt-llama' : '/home/sandernoels/fingeit/data/final/responses/nl/fingpt_llama2_responses_897d1bc3-9a03-4a94-abb9-235f9ad150ec.txt',\n",
    "    'pixiu' : '/home/sandernoels/fingeit/data/final/responses/nl/pixiu_responses_1ed952c6-112e-47b1-8491-e2cc7913d21a.txt',\n",
    "}\n",
    "\n",
    "eval_df = load_from_disk('/home/sandernoels/fingeit/data/final/sampled_eval_df_ext')['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, path in models.items():\n",
    "    eval_df[f'prediction_{model}_raw'] = read_lines(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-sentiment'\n",
    "\n",
    "sentiment_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers(sentiment_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    sentiment_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_sentiment import SentimentEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in sentiment.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = sentiment.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_sentiment_score = SentimentEvaluator()._evaluate(new_test_ds['test'])\n",
    "    eval_sentiment_score.metrics\n",
    "\n",
    "    evals[col] = eval_sentiment_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.783196</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.783196</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.724415</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.740101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.742</td>\n",
       "      <td>0.713072</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.729132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.674</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.661714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.632</td>\n",
       "      <td>0.640086</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.644571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.564</td>\n",
       "      <td>0.529673</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.555212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.540</td>\n",
       "      <td>0.497929</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.520434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.454</td>\n",
       "      <td>0.448226</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.466206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.320940</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.276341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.276</td>\n",
       "      <td>0.167742</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.135818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.268</td>\n",
       "      <td>0.140904</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.113287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit            0.790  0.783196     0.790     0.790308\n",
       "prediction_fingeit_raw        0.790  0.783196     0.790     0.790308\n",
       "prediction_gpt-3.5-turbo      0.752  0.724415     0.752     0.740101\n",
       "prediction_gpt-3.5-turbo_raw  0.742  0.713072     0.742     0.729132\n",
       "prediction_geitje-ultra       0.674  0.638646     0.674     0.661714\n",
       "prediction_pixiu              0.632  0.640086     0.632     0.644571\n",
       "prediction_geitje-ultra_raw   0.564  0.529673     0.564     0.555212\n",
       "prediction_geitje             0.540  0.497929     0.540     0.520434\n",
       "prediction_geitje_raw         0.454  0.448226     0.454     0.466206\n",
       "prediction_fingpt-llama       0.350  0.320940     0.350     0.276341\n",
       "prediction_pixiu_raw          0.276  0.167742     0.276     0.135818\n",
       "prediction_fingpt-llama_raw   0.268  0.140904     0.268     0.113287"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-headline'\n",
    "\n",
    "headline_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers(headline_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    headline_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_headline import HeadlineEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in headline.columns if el.startswith('prediction')]\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    df = headline.copy().fillna('')\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_headline_score = HeadlineEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_headline_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1 binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.459016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.640</td>\n",
       "      <td>0.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.606</td>\n",
       "      <td>0.466125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.314</td>\n",
       "      <td>0.215103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.298</td>\n",
       "      <td>0.166271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.082</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.064</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Acc  F1 binary\n",
       "prediction_fingeit            0.920   0.836066\n",
       "prediction_fingeit_raw        0.920   0.836066\n",
       "prediction_fingpt-llama       0.696   0.000000\n",
       "prediction_pixiu              0.670   0.459016\n",
       "prediction_gpt-3.5-turbo      0.640   0.485714\n",
       "prediction_gpt-3.5-turbo_raw  0.606   0.466125\n",
       "prediction_geitje_raw         0.314   0.215103\n",
       "prediction_geitje             0.298   0.166271\n",
       "prediction_geitje-ultra_raw   0.082   0.068966\n",
       "prediction_geitje-ultra       0.064   0.025000\n",
       "prediction_fingpt-llama_raw   0.000   0.000000\n",
       "prediction_pixiu_raw          0.000   0.000000"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "\n",
    "df.sort_values(by='Acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-ner'\n",
    "\n",
    "ner_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers_ner(ner_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    ner_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner import NEREvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sandernoels/miniconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "prediction_cols = [el for el in ner.columns if el.startswith('prediction')]\n",
    "\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = ner.copy()\n",
    "    df[col] = df[col].astype(str).fillna('nan')\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_ner_score = NEREvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_ner_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.432836</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.417266</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.315217</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.253846</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.154472</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.099585</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.010695</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    F1  \\\n",
       "prediction_fingeit_raw        0.432836   \n",
       "prediction_fingeit            0.417266   \n",
       "prediction_gpt-3.5-turbo      0.315217   \n",
       "prediction_pixiu              0.253846   \n",
       "prediction_geitje             0.154472   \n",
       "prediction_geitje-ultra       0.099585   \n",
       "prediction_fingpt-llama       0.010695   \n",
       "prediction_geitje-ultra_raw        0.0   \n",
       "prediction_geitje_raw              0.0   \n",
       "prediction_fingpt-llama_raw        0.0   \n",
       "prediction_gpt-3.5-turbo_raw       0.0   \n",
       "prediction_pixiu_raw               0.0   \n",
       "\n",
       "                                                          Classification Report  \n",
       "prediction_fingeit_raw                      precision    recall  f1-score   ...  \n",
       "prediction_fingeit                          precision    recall  f1-score   ...  \n",
       "prediction_gpt-3.5-turbo                    precision    recall  f1-score   ...  \n",
       "prediction_pixiu                            precision    recall  f1-score   ...  \n",
       "prediction_geitje                           precision    recall  f1-score   ...  \n",
       "prediction_geitje-ultra                     precision    recall  f1-score   ...  \n",
       "prediction_fingpt-llama                     precision    recall  f1-score   ...  \n",
       "prediction_geitje-ultra_raw                 precision    recall  f1-score   ...  \n",
       "prediction_geitje_raw                       precision    recall  f1-score   ...  \n",
       "prediction_fingpt-llama_raw                 precision    recall  f1-score   ...  \n",
       "prediction_gpt-3.5-turbo_raw                precision    recall  f1-score   ...  \n",
       "prediction_pixiu_raw                        precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER (CLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-ner-cls'\n",
    "\n",
    "ner_cls_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers(ner_cls_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client, ['organisatie', 'locatie', 'persoon'])\n",
    "    ner_cls_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_cls_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_ner_cls import NERCLSEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_cls = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in ner_cls.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = ner_cls.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_ner_cls_classification_score = NERCLSEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_ner_cls_classification_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.890392</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.915893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.814938</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.850624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.776</td>\n",
       "      <td>0.745935</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.791732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo</th>\n",
       "      <td>0.690</td>\n",
       "      <td>0.675122</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.711552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.655376</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.692576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.662</td>\n",
       "      <td>0.649341</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.684253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.386</td>\n",
       "      <td>0.382242</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.356993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.238</td>\n",
       "      <td>0.192246</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.091509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.238</td>\n",
       "      <td>0.192246</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.091509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.238</td>\n",
       "      <td>0.192246</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.091509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.236</td>\n",
       "      <td>0.190939</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.090887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.125212</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.089401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit            0.912  0.890392     0.912     0.915893\n",
       "prediction_fingeit_raw        0.840  0.814938     0.840     0.850624\n",
       "prediction_pixiu              0.776  0.745935     0.776     0.791732\n",
       "prediction_gpt-3.5-turbo      0.690  0.675122     0.690     0.711552\n",
       "prediction_geitje             0.670  0.655376     0.670     0.692576\n",
       "prediction_geitje-ultra       0.662  0.649341     0.662     0.684253\n",
       "prediction_fingpt-llama       0.386  0.382242     0.386     0.356993\n",
       "prediction_geitje-ultra_raw   0.238  0.192246     0.238     0.091509\n",
       "prediction_gpt-3.5-turbo_raw  0.238  0.192246     0.238     0.091509\n",
       "prediction_geitje_raw         0.238  0.192246     0.238     0.091509\n",
       "prediction_pixiu_raw          0.236  0.190939     0.236     0.090887\n",
       "prediction_fingpt-llama_raw   0.222  0.125212     0.222     0.089401"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-finred'\n",
    "\n",
    "finred_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers_finred(finred_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    finred_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred_base.to_csv(f'{data_folder}{task}-cls.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_finred import FinRedEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "finred = pd.read_csv(f'{data_folder}{task}-cls.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in finred.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = finred.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_finred_classification_score = FinRedEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_finred_classification_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.468741</td>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.559711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.479854</td>\n",
       "      <td>0.569277</td>\n",
       "      <td>0.557941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.179006</td>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.232336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.123494</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>0.123494</td>\n",
       "      <td>0.147762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.078931</td>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.121220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.045181</td>\n",
       "      <td>0.040753</td>\n",
       "      <td>0.045181</td>\n",
       "      <td>0.056767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.014581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.005390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   acc  f1_macro  f1_micro  f1_weighted\n",
       "prediction_fingeit_raw        0.569277  0.468741  0.569277     0.559711\n",
       "prediction_fingeit            0.569277  0.479854  0.569277     0.557941\n",
       "prediction_fingpt-llama       0.259036  0.179006  0.259036     0.232336\n",
       "prediction_geitje             0.123494  0.104194  0.123494     0.147762\n",
       "prediction_geitje-ultra       0.111446  0.078931  0.111446     0.121220\n",
       "prediction_pixiu              0.045181  0.040753  0.045181     0.056767\n",
       "prediction_fingpt-llama_raw   0.021084  0.028900  0.021084     0.014581\n",
       "prediction_pixiu_raw          0.003012  0.001949  0.003012     0.005390\n",
       "prediction_geitje-ultra_raw   0.000000  0.000000  0.000000     0.000000\n",
       "prediction_geitje_raw         0.000000  0.000000  0.000000     0.000000\n",
       "prediction_gpt-3.5-turbo_raw  0.000000  0.000000  0.000000     0.000000"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='acc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvFinQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'fingpt-convfinqa'\n",
    "\n",
    "convfinqa_base = eval_df[eval_df['task'] == task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, _ in tqdm(models.items()):\n",
    "    extracted_answer = extracted_answers_convfinqa(convfinqa_base.rename(columns={f'prediction_{model}_raw' : 'prediction'}), client)\n",
    "    convfinqa_base[f'prediction_{model}'] = extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convfinqa_base.to_csv(f'{data_folder}{task}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator_convfinqa import ConvFinQaEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "convfinqa = pd.read_csv(f'{data_folder}{task}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = [el for el in convfinqa.columns if el.startswith('prediction')]\n",
    "\n",
    "evals = {}\n",
    "\n",
    "# eval\n",
    "for col in prediction_cols:\n",
    "    \n",
    "    df = convfinqa.copy()\n",
    "    df = df.rename(columns={col : 'prediction'})\n",
    "    \n",
    "    new_test_ds = DatasetDict()\n",
    "    new_test_ds['test'] = Dataset.from_pandas(df)\n",
    "\n",
    "    eval_convfinqa_score = ConvFinQaEvaluator()._evaluate(new_test_ds['test'])\n",
    "\n",
    "    evals[col] = eval_convfinqa_score.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit_raw</th>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingeit</th>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu_raw</th>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_pixiu</th>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_gpt-3.5-turbo_raw</th>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje</th>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje_raw</th>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra</th>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_geitje-ultra_raw</th>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama_raw</th>\n",
       "      <td>0.010554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_fingpt-llama</th>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy\n",
       "prediction_fingeit_raw        0.324000\n",
       "prediction_fingeit            0.324000\n",
       "prediction_pixiu_raw          0.294000\n",
       "prediction_pixiu              0.286000\n",
       "prediction_gpt-3.5-turbo_raw  0.196000\n",
       "prediction_geitje             0.056000\n",
       "prediction_geitje_raw         0.056000\n",
       "prediction_geitje-ultra       0.036000\n",
       "prediction_geitje-ultra_raw   0.016000\n",
       "prediction_fingpt-llama_raw   0.010554\n",
       "prediction_fingpt-llama       0.008000"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = {key: {metric.name: metric.value for metric in value} for key, value in evals.items()}\n",
    "df = pd.DataFrame(metrics_dict).T\n",
    "df.sort_values(by='Accuracy', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
